{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1111676,"sourceType":"datasetVersion","datasetId":623289}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install git+https://github.com/openai/CLIP.git","metadata":{"trusted":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torch.nn.functional as F\nimport torch.distributions as distributions\nimport torch.optim as optim\n\nimport torchvision\nimport torchvision.transforms as transforms\nimport torchvision.utils as utils\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nclass PlanarFlow(nn.Module):\n    def __init__(self, dim):\n        \"\"\"Instantiates one step of planar flow.\n\n        Reference:\n        Variational Inference with Normalizing Flows\n        Danilo Jimenez Rezende, Shakir Mohamed\n        (https://arxiv.org/abs/1505.05770)\n\n        Args:\n            dim: input dimensionality.\n        \"\"\"\n        super(PlanarFlow, self).__init__()\n\n        self.u = nn.Parameter(torch.randn(1, dim))\n        self.w = nn.Parameter(torch.randn(1, dim))\n        self.b = nn.Parameter(torch.randn(1))\n\n    def forward(self, x):\n        \"\"\"Forward pass.\n\n        Args:\n            x: input tensor (B x D).\n        Returns:\n            transformed x and log-determinant of Jacobian.\n        \"\"\"\n        def m(x):\n            return F.softplus(x) - 1.\n        def h(x):\n            return torch.tanh(x)\n        def h_prime(x):\n            return 1. - h(x)**2\n\n        inner = (self.w * self.u).sum()\n        u = self.u + (m(inner) - inner) * self.w / self.w.norm()**2\n        activation = (self.w * x).sum(dim=1, keepdim=True) + self.b\n        x = x + u * h(activation)\n        psi = h_prime(activation) * self.w\n        log_det = torch.log(torch.abs(1. + (u * psi).sum(dim=1, keepdim=True)))\n\n        return x, log_det\n\nclass RadialFlow(nn.Module):\n    def __init__(self, dim):\n        \"\"\"Instantiates one step of radial flow.\n\n        Reference:\n        Variational Inference with Normalizing Flows\n        Danilo Jimenez Rezende, Shakir Mohamed\n        (https://arxiv.org/abs/1505.05770)\n\n        Args:\n            dim: input dimensionality.\n        \"\"\"\n        super(RadialFlow, self).__init__()\n\n        self.a = nn.Parameter(torch.randn(1))\n        self.b = nn.Parameter(torch.randn(1))\n        self.c = nn.Parameter(torch.randn(1, dim))\n        self.d = dim\n\n    def forward(self, x):\n        \"\"\"Forward pass.\n\n        Args:\n            x: input tensor (B x D).\n        Returns:\n            transformed x and log-determinant of Jacobian.\n        \"\"\"\n        def m(x):\n            return F.softplus(x)\n        def h(r):\n            return 1. / (a + r)\n        def h_prime(r):\n            return -h(r)**2\n\n        a = torch.exp(self.a)\n        b = -a + m(self.b)\n        r = (x - self.c).norm(dim=1, keepdim=True)\n        tmp = b * h(r)\n        x = x + tmp * (x - self.c)\n        log_det = (self.d - 1) * torch.log(1. + tmp) + torch.log(1. + tmp + b * h_prime(r) * r)\n\n        return x, log_det\n\nclass HouseholderFlow(nn.Module):\n    def __init__(self, dim):\n        \"\"\"Instantiates one step of householder flow.\n\n        Reference:\n        Improving Variational Auto-Encoders using Householder Flow\n        Jakub M. Tomczak, Max Welling\n        (https://arxiv.org/abs/1611.09630)\n\n        Args:\n            dim: input dimensionality.\n        \"\"\"\n        super(HouseholderFlow, self).__init__()\n\n        self.v = nn.Parameter(torch.randn(1, dim))\n        self.d = dim\n\n    def forward(self, x):\n        \"\"\"Forward pass.\n\n        Args:\n            x: input tensor (B x D).\n        Returns:\n            transformed x and log-determinant of Jacobian.\n        \"\"\"\n        outer = self.v.t() * self.v\n        v_sqr = self.v.norm()**2\n        H = torch.eye(self.d).cuda() - 2. * outer / v_sqr\n        x = torch.mm(H, x.t()).t()\n\n        return x, 0\n\nclass NiceFlow(nn.Module):\n    def __init__(self, dim, mask, final=False):\n        \"\"\"Instantiates one step of NICE flow.\n\n        Reference:\n        NICE: Non-linear Independent Components Estimation\n        Laurent Dinh, David Krueger, Yoshua Bengio\n        (https://arxiv.org/abs/1410.8516)\n\n        Args:\n            dim: input dimensionality.\n            mask: mask that determines active variables.\n            final: True if the final step, False otherwise.\n        \"\"\"\n        super(NiceFlow, self).__init__()\n\n        self.final = final\n        if final:\n            self.scale = nn.Parameter(torch.zeros(1, dim))\n        else:\n            self.mask = mask\n            self.coupling = nn.Sequential(\n                nn.Linear(dim//2, dim*5), nn.ReLU(),\n                nn.Linear(dim*5, dim*5), nn.ReLU(),\n                nn.Linear(dim*5, dim//2))\n\n    def forward(self, x):\n        if self.final:\n            x = x * torch.exp(self.scale)\n            log_det = torch.sum(self.scale)\n\n            return x, log_det\n        else:\n            [B, W] = list(x.size())\n            x = x.reshape(B, W//2, 2)\n\n            if self.mask:\n                on, off = x[:, :, 0], x[:, :, 1]\n            else:\n                off, on = x[:, :, 0], x[:, :, 1]\n\n            on = on + self.coupling(off)\n\n            if self.mask:\n                x = torch.stack((on, off), dim=2)\n            else:\n                x = torch.stack((off, on), dim=2)\n\n            return x.reshape(B, W), 0\n\nclass Flow(nn.Module):\n    def __init__(self, dim, type, length):\n        \"\"\"Instantiates a chain of flows.\n\n        Args:\n            dim: input dimensionality.\n            type: type of flow.\n            length: length of flow.\n        \"\"\"\n        super(Flow, self).__init__()\n\n        if type == 'planar':\n            self.flow = nn.ModuleList([PlanarFlow(dim) for _ in range(length)])\n        elif type == 'radial':\n            self.flow = nn.ModuleList([RadialFlow(dim) for _ in range(length)])\n        elif type == 'householder':\n            self.flow = nn.ModuleList([HouseholderFlow(dim) for _ in range(length)])\n        elif type == 'nice':\n            self.flow = nn.ModuleList([NiceFlow(dim, i//2, i==(length-1)) for i in range(length)])\n        else:\n            self.flow = nn.ModuleList([])\n\n    def forward(self, x):\n        \"\"\"Forward pass.\n\n        Args:\n            x: input tensor (B x D).\n        Returns:\n            transformed x and log-determinant of Jacobian.\n        \"\"\"\n        [B, _] = list(x.size())\n        log_det = torch.zeros(B, 1).cuda()\n        for i in range(len(self.flow)):\n            x, inc = self.flow[i](x)\n            log_det = log_det + inc\n\n        return x, log_det\n\nclass GatedLayer(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        \"\"\"Instantiates a gated MLP layer.\n\n        Args:\n            in_dim: input dimensionality.\n            out_dim: output dimensionality.\n        \"\"\"\n        super(GatedLayer, self).__init__()\n\n        self.linear = nn.Linear(in_dim, out_dim)\n        self.gate = nn.Sequential(nn.Linear(in_dim, out_dim), nn.Sigmoid())\n\n    def forward(self, x):\n        \"\"\"Forward pass.\n\n        Args:\n            x: input tensor (B x D).\n        Returns:\n            transformed x.\n        \"\"\"\n        return self.linear(x) * self.gate(x)\n\nclass MLPLayer(nn.Module):\n    def __init__(self, in_dim, out_dim, gate):\n        \"\"\"Instantiates an MLP layer.\n\n        Args:\n            in_dim: input dimensionality.\n            out_dim: output dimensionality.\n            gate: whether to use gating mechanism.\n        \"\"\"\n        super(MLPLayer, self).__init__()\n\n        if gate:\n            self.layer = GatedLayer(in_dim, out_dim)\n        else:\n            self.layer = nn.Sequential(nn.Linear(in_dim, out_dim), nn.ReLU())\n\n    def forward(self, x):\n        \"\"\"Forward pass.\n\n        Args:\n            x: input tensor (B x D).\n        Returns:\n            transformed x.\n        \"\"\"\n        return self.layer(x)\n\nclass VAE_with_flow(nn.Module):\n    def __init__(self, dataset, layer, in_dim, hidden_dim, latent_dim, gate, flow, length):\n        \"\"\"Instantiates a VAE.\n\n        Args:\n            dataset: dataset to be modeled.\n            layer: number of hidden layers.\n            in_dim: input dimensionality.\n            hidden_dim: hidden dimensionality.\n            latent_dim: latent dimensionality.\n            gate: whether to use gating mechanism.\n            flow: type of the flow (None if do not use flow).\n            length: length of the flow.\n        \"\"\"\n        super(VAE_with_flow, self).__init__()\n\n        self.dataset = dataset\n        self.latent_dim = latent_dim\n        self.mean = nn.Linear(hidden_dim, latent_dim)\n        self.log_var = nn.Linear(hidden_dim, latent_dim)\n\n        self.encoder = nn.ModuleList(\n            [MLPLayer(in_dim, hidden_dim, gate)] + \\\n            [MLPLayer(hidden_dim, hidden_dim, gate) for _ in range(layer - 1)])\n        self.flow = Flow(latent_dim, flow, length)\n        self.decoder = nn.ModuleList(\n            [MLPLayer(latent_dim, hidden_dim, gate)] + \\\n            [MLPLayer(hidden_dim, hidden_dim, gate) for _ in range(layer - 1)] + \\\n            [nn.Linear(hidden_dim, in_dim)])\n\n    def encode(self, x):\n        \"\"\"Encodes input.\n\n        Args:\n            x: input tensor (B x D).\n        Returns:\n            mean and log-variance of the gaussian approximate posterior.\n        \"\"\"\n        for i in range(len(self.encoder)):\n            x = self.encoder[i](x)\n        return self.mean(x), self.log_var(x)\n\n    def transform(self, mean, log_var):\n        \"\"\"Transforms approximate posterior.\n\n        Args:\n            mean: mean of the gaussian approximate posterior.\n            log_var: log-variance of the gaussian approximate posterior.\n        Returns:\n            transformed latent codes and the log-determinant of the Jacobian.\n        \"\"\"\n        std = torch.exp(.5 * log_var)\n        eps = torch.randn_like(std)\n        z = eps.mul(std).add_(mean)\n\n        return self.flow(z)\n\n    def decode(self, z):\n        \"\"\"Decodes latent codes.\n\n        Args:\n            z: latent codes.\n        Returns:\n            reconstructed input.\n        \"\"\"\n        for i in range(len(self.decoder)):\n            z = self.decoder[i](z)\n        return z\n\n    def sample(self, size):\n        \"\"\"Generates samples from the prior.\n\n        Args:\n            size: number of samples to generate.\n        Returns:\n            generated samples.\n        \"\"\"\n        z = torch.randn(size, self.latent_dim).cuda()\n        if self.dataset == 'mnist':\n            return torch.sigmoid(self.decode(z))\n        else:\n            return self.decode(z)\n\n    def reconstruction_loss(self, x, x_hat):\n        \"\"\"Computes reconstruction loss.\n\n        Args:\n            x: original input (B x D).\n            x_hat: reconstructed input (B x D).\n        Returns:\n            sum of reconstruction loss over the minibatch.\n        \"\"\"\n        if self.dataset == 'mnist':\n            return nn.BCEWithLogitsLoss(reduction='none')(x_hat, x).sum(dim=1, keepdim=True)\n        else:\n            return nn.MSELoss(reduction='none')(x_hat, x).sum(dim=1, keepdim=True)\n\n    def latent_loss(self, mean, log_var, log_det):\n        \"\"\"Computes KL loss.\n\n        Args:\n            mean: mean of the gaussian approximate posterior.\n            log_var: log-variance of the gaussian approximate posterior.\n            log_det: log-determinant of the Jacobian.\n        Returns: sum of KL loss over the minibatch.\n        \"\"\"\n        kl = -.5 * torch.sum(1. + log_var - mean.pow(2) - log_var.exp(), dim=1, keepdim=True)\n        return kl - log_det\n\n    def loss(self, x, x_hat, mean, log_var, log_det):\n        \"\"\"Computes overall loss.\n\n        Args:\n            x: original input (B x D).\n            x_hat: reconstructed input (B x D).\n            mean: mean of the gaussian approximate posterior.\n            log_var: log-variance of the gaussian approximate posterior.\n            log_det: log-determinant of the Jacobian.\n        Returns:\n            sum of reconstruction and KL loss over the minibatch.\n        \"\"\"\n        return self.reconstruction_loss(x, x_hat) + self.latent_loss(mean, log_var, log_det)\n\n    def forward(self, x):\n        \"\"\"Forward pass.\n\n        Args:\n            x: input tensor (B x D).\n        Returns:\n            average loss over the minibatch.\n        \"\"\"\n        mean, log_var = self.encode(x)\n        z, log_det = self.transform(mean, log_var)\n        x_hat = self.decode(z)\n\n        return x_hat, self.loss(x, x_hat, mean, log_var, log_det).mean(),z\n\ndef logit_transform(x, constraint=0.9, reverse=False):\n    '''Transforms data from [0, 1] into unbounded space.\n\n    Restricts data into [0.05, 0.95].\n    Calculates logit(alpha+(1-alpha)*x).\n\n    Args:\n        x: input tensor.\n        constraint: data constraint before logit.\n        reverse: True if transform data back to [0, 1].\n    Returns:\n        transformed tensor and log-determinant of Jacobian from the transform.\n        (if reverse=True, no log-determinant is returned.)\n    '''\n    if reverse:\n        x = 1. / (torch.exp(-x) + 1.)    # [0.05, 0.95]\n        x *= 2.             # [0.1, 1.9]\n        x -= 1.             # [-0.9, 0.9]\n        x /= constraint     # [-1, 1]\n        x += 1.             # [0, 2]\n        x /= 2.             # [0, 1]\n        return x, 0\n    else:\n        [B, C, H, W] = list(x.size())\n\n        # dequantization\n        noise = distributions.Uniform(0., 1.).sample((B, C, H, W))\n        x = (x * 255. + noise) / 256.\n\n        # restrict data\n        x *= 2.             # [0, 2]\n        x -= 1.             # [-1, 1]\n        x *= constraint     # [-0.9, 0.9]\n        x += 1.             # [0.1, 1.9]\n        x /= 2.             # [0.05, 0.95]\n\n        # logit data\n        logit_x = torch.log(x) - torch.log(1. - x)\n\n        # log-determinant of Jacobian from the transform\n        pre_logit_scale = torch.tensor(\n            np.log(constraint) - np.log(1. - constraint))\n        log_diag_J = F.softplus(logit_x) + F.softplus(-logit_x) \\\n            - F.softplus(-pre_logit_scale)\n\n        return logit_x, torch.sum(log_diag_J, dim=(1, 2, 3)).mean()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nfrom types import SimpleNamespace\ndef compute_similarity(img_z,text_z):\n   similarity=F.cosine_similarity(img_z,text_z,dim=1)\n   return similarity\n\nimport clip\nimport os\nimport csv\nimport re\n# optimizer = torch.optim.Adam(list(vae.parameters()) + list(flow.parameters()), lr=1e-4)\ndef get_clip_features(images, texts, clip_model, preprocess, device):\n    # Convert images to PIL, preprocess, and get features\n    images_resized = torch.stack([preprocess(transforms.ToPILImage()(img.cpu())) for img in images])\n    image_features = clip_model.encode_image(images_resized.to(device))\n\n    text_tokens = clip.tokenize(texts).to(device)\n    text_features = clip_model.encode_text(text_tokens)\n\n    return image_features, text_features\n\nclass Flickr8kCaptionDataset(Dataset):\n    def __init__(self, image_folder, captions_dict, transform=None):\n        \"\"\"\n        captions_dict: {image_filename: [caption1, caption2, ...]}\n        We'll pick one caption randomly per sample or you can change as needed.\n        \"\"\"\n        self.image_folder = image_folder\n        self.captions_dict = captions_dict\n        self.transform = transform\n        self.image_files = list(captions_dict.keys())\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        img_file = self.image_files[idx]\n        img_path = os.path.join(self.image_folder, img_file)\n\n        image = Image.open(img_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n\n        # For simplicity, pick the first caption\n        caption = self.captions_dict[img_file][0]\n\n        return image, caption\n\nimport os # Import os module for path manipulation\n\nfrom collections import defaultdict\n\ndef load_captions(caption_file, delimiter=','):\n    \"\"\"\n    Loads image captions from a file, skipping comments and malformed lines.\n\n    Parameters:\n    - caption_file (str): Path to the caption file.\n    - delimiter (str): Delimiter separating filename and caption.\n\n    Returns:\n    - captions_dict (dict): Dictionary mapping filenames to lists of captions.\n    - skipped (int): Number of skipped lines due to formatting issues.\n    \"\"\"\n    captions_dict = defaultdict(list)\n    skipped = 0\n\n    with open(caption_file, 'r', encoding='utf-8') as f:\n        header = next(f)\n        \n        for line_num, line in enumerate(f, start=1):\n            line = line.strip()\n\n            # Skip empty lines and comments\n            if not line or line.startswith('#'):\n                continue\n\n            # Split only on the first comma\n            if delimiter in line:\n                filename, caption = line.split(delimiter, 1)\n                filename = filename.strip()\n                caption = caption.strip()\n\n                if filename and caption:\n                    captions_dict[filename].append(caption)\n                else:\n                    skipped += 1\n                    print(f\"Line {line_num} skipped (missing filename or caption): {line}\")\n            else:\n                skipped += 1\n                print(f\"Line {line_num} skipped (no delimiter found): {line}\")\n\n    return dict(captions_dict), skipped\n\ncaptions,skipped = load_captions(\"/kaggle/input/flickr8k/captions.txt\")\n\ndef main(args):\n\n    device = torch.device(device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using {device} device\")\n    # model hyperparameters\n    dataset_name = args.dataset\n    batch_size = args.batch_size\n    layer = args.layer\n    hidden_dim = args.hidden_dim\n    latent_dim = args.latent_dim\n    gate = args.gate\n    flow = args.flow\n    length = args.length\n\n    clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n\n    # optimization hyperparameters\n    lr = args.lr\n    momentum = args.momentum\n    decay = args.decay\n\n    from torchvision import transforms\n\n    transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n   ])\n    from torch.utils.data import DataLoader\n\n    image_folder = \"/kaggle/input/flickr8k/Images\"\n\n    dataset = Flickr8kCaptionDataset(image_folder, captions, transform=transform)\n    dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=2)\n\n    # for images, texts in dataloader:\n    #     image_features, text_features = get_clip_features(images, texts, clip_model, preprocess, device)\n\n    vae_image = VAE_with_flow(dataset, layer, 512, hidden_dim, latent_dim, gate, flow, length).to(device)\n    vae_text = VAE_with_flow(dataset, layer, 512, hidden_dim, latent_dim, gate, flow, length).to(device)\n    optimizer = optim.Adam(list(vae_image.parameters())+list(vae_text.parameters()), lr=lr, betas=(momentum, decay))\n    total_iter = 0\n\n\n    running_loss = 0.0\n    from tqdm import tqdm\n    for epoch in range(1, 100):\n        for images,texts in tqdm(dataloader, desc=f\"Epoch {epoch}\"):\n            image_features, text_features = get_clip_features(images, texts, clip_model, preprocess, device)\n            vae_image.train()\n            if total_iter == args.max_iter:\n                break\n\n            total_iter += 1\n            optimizer.zero_grad()\n            image_features = image_features / image_features.norm(dim=1, keepdim=True)\n\n            # forward pass\n            x_in_image=image_features.to(dtype=torch.float32)\n            x_in_text=text_features.to(dtype=torch.float32)\n            x_hat_image, loss_image, z_image = vae_image(x_in_image)\n            x_hat_text, loss_text, z_text = vae_text(x_in_text)\n            similarity=compute_similarity(z_image, z_text)\n            loss = loss_image + loss_text\n            running_loss += loss.item()\n\n        \n            loss.backward()\n            optimizer.step()\n\n            if total_iter % 1000 == 0:\n                mean_loss = running_loss / 1000\n                print('iter %s:' % total_iter,\n                      'loss = %.3f' % mean_loss,\n                      )\n                running_loss = 0.0\n                with torch.no_grad():\n                # Cosine similarity between image and text latent vectors\n                  cos = torch.nn.CosineSimilarity(dim=1)\n                  sim_scores = cos(z_image, z_text)\n                  avg_sim = sim_scores.mean().item()\n\n                # Reconstruction quality (optional): MSE or MAE\n                  mse_image = torch.nn.functional.mse_loss(x_hat_image, x_in_image).item()\n                  mse_text = torch.nn.functional.mse_loss(x_hat_text, x_in_text).item()\n\n                print(f\"[Iter {total_iter}] Loss: {loss.item():.4f} | Cosine Sim: {avg_sim:.4f} | MSE (img): {mse_image:.4f} | MSE (txt): {mse_text:.4f}\")\n\n\n                if total_iter % 20000 == 0:\n                    torch.save({\n                        'total_iter': total_iter,\n                        'loss': mean_loss,\n                        'vae_image_state_dict': vae_image.state_dict(),\n                        'vae_text_state_dict': vae_text.state_dict(),\n                        'optimizer_state_dict': optimizer.state_dict(),\n                        'batch_size': batch_size,\n                        'layer': layer,\n                        'hidden_dim': hidden_dim,\n                        'latent_dim': latent_dim,\n                        'gate': gate,\n                        'flow': flow,\n                        'length': length,\n                        'similarity':similarity\n                        }\n                        )\n                    print('Checkpoint saved.')\n\n    print('Training finished.')\n\nif __name__ == '__main__':\n    args = {\n    'dataset': 'flickr8k',\n    'batch_size': 32,\n    'layer': 3,\n    'hidden_dim': 512,\n    'latent_dim': 256,\n    'gate': 1,\n    'flow': None,\n    'length': 2,\n    'lr': 1e-4,\n    'momentum': 0.9,\n    'decay': 0.990,\n    'max_iter': 100000,\n    'sample_size': 16\n}\n\nargs = SimpleNamespace(**args)\nmain(args)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}